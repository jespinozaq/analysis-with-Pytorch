# -*- coding: utf-8 -*-
"""Project10Espinoza.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tHhW0n-D4NIoakhA5fnzEL9rQKBMX-FG
"""

#Jesus Espinoza
#Project 10? 11? Last project yay!
#Purpose of project: create a neural network using MNIST dataset
#Followed the step by step process shown on this website: https://www.tensorflow.org/tutorials/quickstart/beginner

#importing TensorFlow into my program

import tensorflow as tf
print("TensorFlow version:", tf.__version__)

#Loading in a dataset from MNIST

mnist = tf.keras.datasets.mnist

#converting sample data from ints to floating points for x and y train and test
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

#Building a sequential model by stacking layers. Here I messed around with the dropout and dense values.
#The best values I have found so far are .1 for the first dropout, .3 for the second and then for dense was 25.
#However, for the most part the values do not seem to vary much throughout the different numbers I used.

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.15),
  tf.keras.layers.Dense(25),
  tf.keras.layers.Dropout(0.15)
])

#The model here returns vector scores for each class within it.

predictions = model(x_train[:1]).numpy()
predictions

#Here we are converting the vector scores to probabilities for each class within it.

tf.nn.softmax(predictions).numpy()

#Here we are defining a loss function from the probability vectors and returns a scalar for the loss in each example. 
#The loss is then equal to the negative log probability of the "true" class. The instruction model says it should be super close to 2.3

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
loss_fn(y_train[:1], predictions).numpy()

#The TensorFlow instructions model has values that are way closer to 2.3 than mine. 
#The numbers that you are seeing so far are from my runs after following the instructions to see how much it could vary.
#When I first started diverging from the original numbers done in the instruction model, I was hovering around 4-4.5 loss values.
#Once I started going closer to what the instruction model had, the numbers lowered again. 
#I'm going to research how these numbers came to be, I wonder if it was just guessing and seeing the result and then making a more educated guess from that.

#For compiling, the optimizer class is set to adam, the loss function is set to the function defined 2 cells above, and we set the metric parameter to focus on accuracy.

model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])

#Here I begin training the model.

model.fit(x_train, y_train, epochs=20)

#Here I test/evaluate the model that was trained above. I was averging around 90-95% accuracy when I was messing around and deviating from the instruction module. This was the closest I could get to what the module had.

model.evaluate(x_test,  y_test, verbose=2)